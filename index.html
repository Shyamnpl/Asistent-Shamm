<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Vision Assistant</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700&display=swap');
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Poppins', sans-serif;
        }
        
        body {
            overflow: hidden;
            background: linear-gradient(135deg, #1a2a6c, #b21f1f, #fdbb2d);
            height: 100vh;
            width: 100vw;
            position: relative;
        }
        
        .glassmorphism {
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(12px);
            border-radius: 20px;
            border: 1px solid rgba(255, 255, 255, 0.2);
            box-shadow: 0 8px 32px 0 rgba(31, 38, 135, 0.37);
        }
        
        .fold-curve {
            border-radius: 0 0 40px 40px;
        }
        
        .camera-frame {
            aspect-ratio: 3/4;
            max-height: 70vh;
            border-radius: 20px;
            overflow: hidden;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3);
        }
        
        .mic-active {
            animation: pulse 1.5s infinite;
            background: rgba(0, 255, 180, 0.3);
        }
        
        @keyframes pulse {
            0% { 
                box-shadow: 0 0 0 0 rgba(0, 255, 180, 0.6); 
            }
            70% { 
                box-shadow: 0 0 0 20px rgba(0, 255, 180, 0); 
            }
            100% { 
                box-shadow: 0 0 0 0 rgba(0, 255, 180, 0); 
            }
        }
        
        .ai-speaking {
            animation: gentleGlow 2s infinite alternate;
        }
        
        @keyframes gentleGlow {
            0% { 
                box-shadow: 0 0 10px rgba(255, 255, 255, 0.3);
            }
            100% { 
                box-shadow: 0 0 20px rgba(255, 255, 255, 0.6);
            }
        }
        
        .detection-box {
            position: absolute;
            border: 2px solid #00ff88;
            background: rgba(0, 255, 136, 0.1);
            pointer-events: none;
        }
        
        .detection-label {
            position: absolute;
            background: rgba(0, 0, 0, 0.7);
            color: white;
            padding: 2px 6px;
            font-size: 12px;
            border-radius: 4px;
            pointer-events: none;
        }

        .error-message {
            background: rgba(255, 0, 0, 0.2);
            border: 1px solid rgba(255, 0, 0, 0.5);
        }
    </style>
</head>
<body class="flex flex-col items-center justify-center min-h-screen text-white p-4">
    
    <div class="w-full max-w-md h-full max-h-[90vh] flex flex-col z-10">
        <!-- Header with AI Status -->
        <div class="glassmorphism p-4 mb-4 flex items-center justify-between fold-curve">
            <div class="flex items-center">
                <div class="w-12 h-12 rounded-full bg-gradient-to-r from-cyan-400 to-blue-500 flex items-center justify-center ai-speaking">
                    <i class="fas fa-robot text-white text-xl"></i>
                </div>
                <div class="ml-3">
                    <h1 class="text-lg font-bold">Vision Assistant</h1>
                    <p class="text-xs opacity-80">AI + Object Detection</p>
                </div>
            </div>
            
            <div id="statusIndicator" class="flex items-center bg-black bg-opacity-30 px-3 py-1 rounded-full">
                <div class="w-2 h-2 bg-green-400 rounded-full mr-2"></div>
                <span class="text-xs">Ready</span>
            </div>
        </div>
        
        <!-- Camera Preview Area -->
        <div class="flex-grow flex items-center justify-center mb-4 relative">
            <div class="camera-frame glassmorphism w-full h-full relative overflow-hidden">
                <video id="cameraView" autoplay playsinline class="w-full h-full object-cover"></video>
                <canvas id="detectionCanvas" class="absolute top-0 left-0 w-full h-full pointer-events-none"></canvas>
                
                <!-- Camera overlay -->
                <div class="absolute bottom-0 left-0 right-0 bg-gradient-to-t from-black to-transparent p-4">
                    <p class="text-center text-sm" id="cameraStatus">Camera ready for object detection</p>
                </div>
            </div>
        </div>
        
        <!-- Detection Results -->
        <div id="detectionResults" class="glassmorphism p-3 mb-4 rounded-xl hidden">
            <h3 class="text-sm font-bold mb-2">Detected Objects:</h3>
            <div id="objectsList" class="text-xs"></div>
        </div>

        <!-- Error Display -->
        <div id="errorDisplay" class="error-message p-3 mb-4 rounded-xl hidden">
            <p id="errorText" class="text-sm"></p>
        </div>
        
        <!-- Voice Status -->
        <div id="voiceStatus" class="glassmorphism p-4 mb-4 text-center rounded-xl">
            <p id="statusText" class="text-lg">Tap mic to speak</p>
            <p id="userSpeechText" class="text-sm mt-2 opacity-80 min-h-[20px]"></p>
        </div>
        
        <!-- Bottom Controls -->
        <div class="glassmorphism p-6 flex justify-between items-center fold-curve">
            <button id="cameraButton" class="w-14 h-14 rounded-full bg-white bg-opacity-20 flex items-center justify-center transition-all duration-300 hover:bg-opacity-30">
                <i class="fas fa-camera text-white text-xl"></i>
            </button>
            
            <button id="micButton" class="w-20 h-20 rounded-full bg-gradient-to-r from-cyan-500 to-blue-500 flex items-center justify-center transition-all duration-300 hover:from-cyan-600 hover:to-blue-600">
                <i class="fas fa-microphone text-white text-2xl"></i>
            </button>
            
            <button id="detectButton" class="w-14 h-14 rounded-full bg-white bg-opacity-20 flex items-center justify-center transition-all duration-300 hover:bg-opacity-30">
                <i class="fas fa-eye text-white text-xl"></i>
            </button>
        </div>
    </div>

    <script>
        // DOM Elements
        const micButton = document.getElementById('micButton');
        const cameraButton = document.getElementById('cameraButton');
        const detectButton = document.getElementById('detectButton');
        const cameraView = document.getElementById('cameraView');
        const detectionCanvas = document.getElementById('detectionCanvas');
        const statusIndicator = document.getElementById('statusIndicator');
        const statusText = document.getElementById('statusText');
        const userSpeechText = document.getElementById('userSpeechText');
        const detectionResults = document.getElementById('detectionResults');
        const objectsList = document.getElementById('objectsList');
        const cameraStatus = document.getElementById('cameraStatus');
        const errorDisplay = document.getElementById('errorDisplay');
        const errorText = document.getElementById('errorText');
        
        // App State
        let isListening = false;
        let isCameraActive = false;
        let usingFrontCamera = false;
        let stream = null;
        let isAISpeaking = false;
        let detectionModel = null;
        let isDetecting = false;
        let apiRetryCount = 0;
        const MAX_RETRIES = 3;
        
        // API Configuration
        const HF_TOKEN = "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";
        const MODEL = "deepseek-ai/DeepSeek-V3.2-Exp";
        
        // Show error message
        function showError(message) {
            errorText.textContent = message;
            errorDisplay.classList.remove('hidden');
            setTimeout(() => {
                errorDisplay.classList.add('hidden');
            }, 5000);
        }

        // TensorFlow.js Model
        async function loadDetectionModel() {
            try {
                statusText.textContent = "Loading AI model...";
                detectionModel = await cocoSsd.load();
                statusText.textContent = "AI model loaded! Tap mic to speak";
                console.log("COCO-SSD model loaded successfully");
                return true;
            } catch (error) {
                console.error('Error loading detection model:', error);
                showError("Error loading AI model");
                statusText.textContent = "Error loading AI model";
                return false;
            }
        }
        
        // Object Detection
        async function detectObjects() {
            if (!detectionModel || !isCameraActive) {
                statusText.textContent = "Camera or AI model not ready";
                return;
            }
            
            try {
                isDetecting = true;
                detectButton.style.background = 'rgba(0, 255, 136, 0.3)';
                statusText.textContent = "Detecting objects...";
                
                const predictions = await detectionModel.detect(cameraView);
                
                // Clear previous detections
                const ctx = detectionCanvas.getContext('2d');
                ctx.clearRect(0, 0, detectionCanvas.width, detectionCanvas.height);
                
                // Set canvas size to match video
                detectionCanvas.width = cameraView.videoWidth;
                detectionCanvas.height = cameraView.videoHeight;
                
                let detectedObjects = [];
                
                // Draw detection boxes
                predictions.forEach(prediction => {
                    const [x, y, width, height] = prediction.bbox;
                    const score = (prediction.score * 100).toFixed(1);
                    
                    // Draw bounding box
                    ctx.strokeStyle = '#00ff88';
                    ctx.lineWidth = 2;
                    ctx.strokeRect(x, y, width, height);
                    
                    // Draw label background
                    ctx.fillStyle = 'rgba(0, 0, 0, 0.7)';
                    const text = `${prediction.class} ${score}%`;
                    const textWidth = ctx.measureText(text).width;
                    ctx.fillRect(x, y - 20, textWidth + 10, 20);
                    
                    // Draw label text
                    ctx.fillStyle = '#00ff88';
                    ctx.font = '14px Arial';
                    ctx.fillText(text, x + 5, y - 5);
                    
                    detectedObjects.push({
                        class: prediction.class,
                        score: score,
                        confidence: prediction.score
                    });
                });
                
                // Update detection results
                if (detectedObjects.length > 0) {
                    objectsList.innerHTML = detectedObjects.map(obj => 
                        `<div class="flex justify-between mb-1">
                            <span>${obj.class}</span>
                            <span>${obj.score}%</span>
                        </div>`
                    ).join('');
                    detectionResults.classList.remove('hidden');
                    cameraStatus.textContent = `Detected ${detectedObjects.length} objects`;
                } else {
                    detectionResults.classList.add('hidden');
                    cameraStatus.textContent = "No objects detected";
                }
                
                statusText.textContent = `Detected ${detectedObjects.length} objects`;
                return detectedObjects;
                
            } catch (error) {
                console.error('Detection error:', error);
                showError("Object detection failed");
                statusText.textContent = "Detection failed";
            } finally {
                isDetecting = false;
                detectButton.style.background = '';
            }
        }
        
        // Speech Recognition
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        let recognition;
        
        if (SpeechRecognition) {
            recognition = new SpeechRecognition();
            recognition.continuous = false;
            recognition.interimResults = false;
            recognition.lang = 'en-US';
            
            recognition.onresult = (event) => {
                const text = event.results[0][0].transcript;
                userSpeechText.textContent = `You said: "${text}"`;
                handleUserSpeech(text);
            };
            
            recognition.onstart = () => {
                isListening = true;
                micButton.classList.add('mic-active');
                statusText.textContent = "Listening...";
            };
            
            recognition.onend = () => {
                isListening = false;
                micButton.classList.remove('mic-active');
            };
            
            recognition.onerror = (event) => {
                console.error('Speech recognition error:', event.error);
                isListening = false;
                micButton.classList.remove('mic-active');
                showError("Speech recognition error");
                statusText.textContent = "Error listening";
            };
        }
        
        // Camera Management
        async function startCamera(front = false) {
            try {
                if (stream) {
                    stream.getTracks().forEach(track => track.stop());
                }
                
                const constraints = {
                    video: { 
                        facingMode: front ? "user" : "environment",
                        aspectRatio: 3/4
                    },
                    audio: false
                };
                
                stream = await navigator.mediaDevices.getUserMedia(constraints);
                cameraView.srcObject = stream;
                isCameraActive = true;
                
                // Set canvas size when video metadata is loaded
                cameraView.onloadedmetadata = () => {
                    detectionCanvas.width = cameraView.videoWidth;
                    detectionCanvas.height = cameraView.videoHeight;
                };
                
                statusText.textContent = "Camera active";
                cameraStatus.textContent = "Ready for object detection";
                
            } catch (error) {
                console.error('Error accessing camera:', error);
                showError("Camera access denied");
                statusText.textContent = "Camera access denied";
            }
        }
        
        function switchCamera() {
            usingFrontCamera = !usingFrontCamera;
            startCamera(usingFrontCamera);
        }
        
        // Hugging Face API with Better Error Handling
        async function queryDeepSeekTextOnly(prompt) {
            try {
                console.log("Sending request to Hugging Face API...");
                
                // Add timeout to prevent hanging requests
                const controller = new AbortController();
                const timeoutId = setTimeout(() => controller.abort(), 10000); // 10 second timeout
                
                const response = await fetch(`https://api-inference.huggingface.co/models/${MODEL}`, {
                    method: "POST",
                    headers: {
                        "Authorization": `Bearer ${HF_TOKEN}`,
                        "Content-Type": "application/json"
                    },
                    body: JSON.stringify({ 
                        inputs: prompt,
                        parameters: {
                            max_new_tokens: 150,
                            temperature: 0.7,
                            do_sample: true
                        }
                    }),
                    signal: controller.signal
                });

                clearTimeout(timeoutId);

                if (!response.ok) {
                    const errorData = await response.text();
                    console.error('API Response Error:', errorData);
                    
                    if (response.status === 503) {
                        throw new Error("Model is loading, please try again in a few seconds");
                    } else if (response.status === 429) {
                        throw new Error("Too many requests, please wait a moment");
                    } else if (response.status === 401) {
                        throw new Error("API token invalid");
                    } else {
                        throw new Error(`API error: ${response.status} ${response.statusText}`);
                    }
                }

                const result = await response.json();
                
                if (result.error) {
                    throw new Error(result.error);
                }

                let generatedText = "";
                if (Array.isArray(result) && result.length > 0) {
                    generatedText = result[0].generated_text || "I'm not sure how to respond to that.";
                } else if (result.generated_text) {
                    generatedText = result.generated_text;
                } else {
                    generatedText = "I received your message but didn't generate a proper response.";
                }

                // Reset retry count on successful request
                apiRetryCount = 0;
                return generatedText;

            } catch (error) {
                console.error('Hugging Face API error:', error);
                
                // Increment retry count
                apiRetryCount++;
                
                if (apiRetryCount >= MAX_RETRIES) {
                    throw new Error("AI service is temporarily unavailable. Please try again later.");
                }
                
                throw error;
            }
        }
        
        // Enhanced AI query with vision context
        async function queryWithVisionContext(userPrompt) {
            let visionContext = "";
            
            if (isCameraActive && detectionModel) {
                const detectedObjects = await detectObjects();
                if (detectedObjects && detectedObjects.length > 0) {
                    const objectsText = detectedObjects.map(obj => obj.class).join(', ');
                    visionContext = ` The user is showing me: ${objectsText}.`;
                }
            }
            
            const systemPrompt = "You are a helpful AI assistant with computer vision capabilities. Provide helpful, concise responses.";
            const fullPrompt = `${systemPrompt}${visionContext}\n\nUser: ${userPrompt}\n\nAssistant:`;
            
            return await queryDeepSeekTextOnly(fullPrompt);
        }
        
        // Text-to-Speech
        function speakAI(text) {
            return new Promise((resolve) => {
                isAISpeaking = true;
                statusText.textContent = "AI is speaking...";
                
                const utterance = new SpeechSynthesisUtterance(text);
                utterance.rate = 1;
                utterance.pitch = 1;
                utterance.volume = 0.8;
                
                utterance.onend = () => {
                    isAISpeaking = false;
                    statusText.textContent = "Tap mic to speak";
                    resolve();
                };
                
                utterance.onerror = (event) => {
                    console.error('Speech synthesis error:', event);
                    isAISpeaking = false;
                    showError("Speech synthesis failed");
                    statusText.textContent = "Speech error";
                    resolve();
                };
                
                speechSynthesis.speak(utterance);
            });
        }

        // Fallback responses when API fails
        function getFallbackResponse(userText, detectedObjects = []) {
            const lowerText = userText.toLowerCase();
            
            // Greeting responses
            if (lowerText.includes('hello') || lowerText.includes('hi') || lowerText.includes('hey')) {
                return "Hello! I'm your AI assistant. How can I help you today?";
            }
            
            // Vision-related responses
            if (lowerText.includes('see') || lowerText.includes('detect') || lowerText.includes('what') && lowerText.includes('object')) {
                if (detectedObjects.length > 0) {
                    const objects = detectedObjects.map(obj => obj.class).join(', ');
                    return `I can see: ${objects}. What would you like to know about these objects?`;
                } else {
                    return "I don't see any objects clearly at the moment. Try moving the camera or improving the lighting.";
                }
            }
            
            // Time-related
            if (lowerText.includes('time')) {
                return `The current time is ${new Date().toLocaleTimeString()}`;
            }
            
            // Default fallback
            return "I understand you're asking something, but I'm having trouble connecting to my AI service right now. Please try again in a moment.";
        }
        
        // Command Handling with Robust Error Handling
        async function handleUserSpeech(text) {
            try {
                statusText.textContent = "Processing...";
                
                let response;
                let detectedObjects = [];
                
                // Always detect objects for context
                if (isCameraActive && detectionModel) {
                    detectedObjects = await detectObjects() || [];
                }
                
                try {
                    if (text.toLowerCase().includes('what can you see') || 
                        text.toLowerCase().includes('detect') ||
                        text.toLowerCase().includes('what\'s in front')) {
                        response = await queryWithVisionContext(text);
                    } else {
                        response = await queryDeepSeekTextOnly(text);
                    }
                } catch (apiError) {
                    console.log('API failed, using fallback response');
                    response = getFallbackResponse(text, detectedObjects);
                }
                
                await speakAI(response);
                
            } catch (error) {
                console.error('Error processing speech:', error);
                const fallback = getFallbackResponse(text);
                await speakAI(fallback);
            }
        }
        
        // Event Listeners
        micButton.addEventListener('click', () => {
            if (!isListening && !isAISpeaking) {
                if (!SpeechRecognition) {
                    showError("Speech recognition not supported in this browser");
                    return;
                }
                recognition.start();
            }
        });
        
        cameraButton.addEventListener('click', switchCamera);
        
        detectButton.addEventListener('click', detectObjects);
        
        // Auto-detect every 3 seconds when camera is active
        setInterval(() => {
            if (isCameraActive && detectionModel && !isDetecting) {
                detectObjects();
            }
        }, 3000);
        
        // Initialize
        window.addEventListener('load', async () => {
            try {
                await loadDetectionModel();
                await startCamera(false);
                statusText.textContent = "Ready! Tap mic to speak or eye to detect objects";
            } catch (error) {
                showError("Failed to initialize application");
                statusText.textContent = "Initialization failed";
            }
        });

        // Handle page visibility changes
        document.addEventListener('visibilitychange', function() {
            if (document.hidden) {
                speechSynthesis.cancel();
                if (recognition && isListening) {
                    recognition.stop();
                }
            }
        });
    </script>
</body>
</html>