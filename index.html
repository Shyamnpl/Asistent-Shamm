<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Vision Assistant</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700&display=swap');
        
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
            font-family: 'Poppins', sans-serif;
        }
        
        body {
            background: linear-gradient(135deg, #1a2a6c, #b21f1f, #fdbb2d);
            min-height: 100vh;
            width: 100vw;
            position: relative;
            overflow-x: hidden;
        }
        
        .glassmorphism {
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(12px);
            border-radius: 20px;
            border: 1px solid rgba(255, 255, 255, 0.2);
            box-shadow: 0 8px 32px 0 rgba(31, 38, 135, 0.37);
        }
        
        .fold-curve {
            border-radius: 0 0 40px 40px;
        }
        
        .camera-frame {
            aspect-ratio: 3/4;
            max-height: 70vh;
            border-radius: 20px;
            overflow: hidden;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3);
        }
        
        .mic-active {
            animation: pulse 1.5s infinite;
            background: rgba(0, 255, 180, 0.3);
        }
        
        .mic-listening {
            animation: listeningGlow 2s infinite;
            background: rgba(255, 215, 0, 0.3);
        }
        
        .mic-ignoring {
            animation: ignoringGlow 2s infinite;
            background: rgba(255, 100, 0, 0.3);
        }
        
        @keyframes pulse {
            0% { 
                box-shadow: 0 0 0 0 rgba(0, 255, 180, 0.6); 
            }
            70% { 
                box-shadow: 0 0 0 20px rgba(0, 255, 180, 0); 
            }
            100% { 
                box-shadow: 0 0 0 0 rgba(0, 255, 180, 0); 
            }
        }
        
        @keyframes listeningGlow {
            0%, 100% { 
                box-shadow: 0 0 0 0 rgba(255, 215, 0, 0.6);
                transform: scale(1);
            }
            50% { 
                box-shadow: 0 0 0 15px rgba(255, 215, 0, 0.3);
                transform: scale(1.05);
            }
        }
        
        @keyframes ignoringGlow {
            0%, 100% { 
                box-shadow: 0 0 0 0 rgba(255, 100, 0, 0.6);
                transform: scale(1);
            }
            50% { 
                box-shadow: 0 0 0 15px rgba(255, 100, 0, 0.3);
                transform: scale(1.02);
            }
        }
        
        .ai-speaking {
            animation: gentleGlow 2s infinite alternate;
        }
        
        @keyframes gentleGlow {
            0% { 
                box-shadow: 0 0 10px rgba(255, 255, 255, 0.3);
            }
            100% { 
                box-shadow: 0 0 20px rgba(255, 255, 255, 0.6);
            }
        }
        
        .scrollable-container {
            max-height: 80vh;
            overflow-y: auto;
            overflow-x: hidden;
        }

        .scrollable-container::-webkit-scrollbar {
            width: 6px;
        }

        .scrollable-container::-webkit-scrollbar-track {
            background: rgba(255, 255, 255, 0.1);
            border-radius: 10px;
        }

        .scrollable-container::-webkit-scrollbar-thumb {
            background: rgba(255, 255, 255, 0.3);
            border-radius: 10px;
        }

        .scrollable-container::-webkit-scrollbar-thumb:hover {
            background: rgba(255, 255, 255, 0.5);
        }

        .error-message {
            background: rgba(255, 0, 0, 0.2);
            border: 1px solid rgba(255, 0, 0, 0.5);
        }

        .success-message {
            background: rgba(0, 255, 0, 0.2);
            border: 1px solid rgba(0, 255, 0, 0.5);
        }

        .warning-message {
            background: rgba(255, 165, 0, 0.2);
            border: 1px solid rgba(255, 165, 0, 0.5);
        }

        .image-preview {
            max-width: 100%;
            border-radius: 10px;
            margin-top: 10px;
        }

        .read-more-btn {
            background: rgba(255, 255, 255, 0.1);
            border: none;
            color: #00ff88;
            padding: 2px 8px;
            border-radius: 12px;
            font-size: 10px;
            cursor: pointer;
            margin-left: 5px;
        }

        .full-conversation {
            max-height: 200px;
            overflow-y: auto;
            background: rgba(0, 0, 0, 0.3);
            padding: 10px;
            border-radius: 8px;
            margin-top: 5px;
        }

        .download-btn {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            border: none;
            padding: 8px 16px;
            border-radius: 20px;
            cursor: pointer;
            margin-top: 10px;
            font-size: 12px;
        }

        .camera-switch-btn {
            position: absolute;
            top: 10px;
            right: 10px;
            background: rgba(0, 0, 0, 0.5);
            color: white;
            border: none;
            padding: 8px;
            border-radius: 50%;
            cursor: pointer;
            z-index: 10;
        }

        .listening-indicator {
            position: absolute;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(255, 215, 0, 0.8);
            color: black;
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 12px;
            animation: pulse 2s infinite;
        }

        .ignoring-indicator {
            position: absolute;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(255, 100, 0, 0.8);
            color: white;
            padding: 5px 15px;
            border-radius: 20px;
            font-size: 12px;
            animation: pulse 2s infinite;
        }
    </style>
</head>
<body class="flex flex-col items-center min-h-screen text-white p-4 py-8">
    
    <div class="w-full max-w-md flex flex-col z-10 space-y-4">
        <!-- Header with AI Status -->
        <div class="glassmorphism p-4 flex items-center justify-between fold-curve">
            <div class="flex items-center">
                <div class="w-12 h-12 rounded-full bg-gradient-to-r from-cyan-400 to-blue-500 flex items-center justify-center ai-speaking">
                    <i class="fas fa-robot text-white text-xl"></i>
                </div>
                <div class="ml-3">
                    <h1 class="text-lg font-bold">Vision Assistant</h1>
                    <p class="text-xs opacity-80">Smart Listening Mode</p>
                </div>
            </div>
            
            <div id="statusIndicator" class="flex items-center bg-black bg-opacity-30 px-3 py-1 rounded-full">
                <div class="w-2 h-2 bg-green-400 rounded-full mr-2"></div>
                <span class="text-xs" id="statusText">Ready</span>
            </div>
        </div>
        
        <!-- Main Content Area -->
        <div class="scrollable-container space-y-4">
            <!-- Camera Preview Area -->
            <div class="flex items-center justify-center relative">
                <div class="camera-frame glassmorphism w-full relative overflow-hidden">
                    <video id="cameraView" autoplay playsinline class="w-full h-full object-cover"></video>
                    <canvas id="detectionCanvas" class="absolute top-0 left-0 w-full h-full pointer-events-none"></canvas>
                    
                    <!-- Camera Switch Button -->
                    <button id="cameraSwitchBtn" class="camera-switch-btn">
                        <i class="fas fa-camera-rotate"></i>
                    </button>
                    
                    <!-- Listening Indicator -->
                    <div id="listeningIndicator" class="listening-indicator hidden">
                        üé§ Listening for your voice...
                    </div>
                    
                    <!-- Ignoring Indicator -->
                    <div id="ignoringIndicator" class="ignoring-indicator hidden">
                        üîá Ignoring AI sounds...
                    </div>
                    
                    <!-- Camera overlay -->
                    <div class="absolute bottom- 0 left-0 right-0 bg-gradient-to-t from-black to-transparent p-4">
                        <p class="text-center text-sm" id="cameraStatus">Camera ready - Show objects and ask questions</p>
                    </div>
                </div>
            </div>
            
            <!-- API Status -->
            <div id="apiStatus" class="success-message p-3 rounded-xl">
                <p class="text-sm">üîó Connected to Gemini 2.0 Flash</p>
            </div>

            <!-- Warning Message -->
            <div id="warningDisplay" class="warning-message p-3 rounded-xl hidden">
                <p id="warningText" class="text-sm"></p>
            </div>

            <!-- Error Display -->
            <div id="errorDisplay" class="error-message p-3 rounded-xl hidden">
                <p id="errorText" class="text-sm"></p>
            </div>
            
            <!-- Voice Status -->
            <div id="voiceStatus" class="glassmorphism p-4 text-center rounded-xl">
                <p id="mainStatusText" class="text-lg">Tap mic to start conversation</p>
                <p id="userSpeechText" class="text-sm mt-2 opacity-80 min-h-[20px]"></p>
            </div>

            <!-- Generated Image Preview -->
            <div id="imagePreviewContainer" class="glassmorphism p-4 rounded-xl hidden">
                <h3 class="text-sm font-bold mb-2">üñºÔ∏è Generated Image</h3>
                <img id="generatedImage" class="image-preview" alt="AI Generated Image">
                <button id="downloadImageBtn" class="download-btn">
                    <i class="fas fa-download"></i> Download Image
                </button>
            </div>

            <!-- Conversation History -->
            <div id="conversationHistory" class="glassmorphism p-4 rounded-xl">
                <h3 class="text-sm font-bold mb-2">üí¨ Recent Conversation</h3>
                <div id="conversationList" class="text-xs space-y-2 max-h-40 overflow-y-auto"></div>
            </div>

            <!-- Instructions -->
            <div class="glassmorphism p-4 rounded-xl">
                <h3 class="text-sm font-bold mb-2">üí° Smart Listening Mode:</h3>
                <ul class="text-xs space-y-1 opacity-80">
                    <li>‚Ä¢ <span class="text-cyan-400">Ignores AI voice</span> - No echo bugs</li>
                    <li>‚Ä¢ <span class="text-cyan-400">Only hears you</span> - System sounds ignored</li>
                    <li>‚Ä¢ <span class="text-cyan-400">Auto image capture</span> for vision questions</li>
                    <li>‚Ä¢ <span class="text-cyan-400">Continuous listening</span> with 1min timeout</li>
                </ul>
            </div>
        </div>
        
        <!-- Bottom Controls -->
        <div class="glassmorphism p-6 flex justify-between items-center fold-curve mt-4">
            <button id="cameraButton" class="w-14 h-14 rounded-full bg-white bg-opacity-20 flex items-center justify-center transition-all duration-300 hover:bg-opacity-30">
                <i class="fas fa-camera text-white text-xl"></i>
            </button>
            
            <button id="micButton" class="w-20 h-20 rounded-full bg-gradient-to-r from-cyan-500 to-blue-500 flex items-center justify-center transition-all duration-300 hover:from-cyan-600 hover:to-blue-600">
                <i class="fas fa-microphone text-white text-2xl"></i>
            </button>
            
            <button id="analyzeButton" class="w-14 h-14 rounded-full bg-white bg-opacity-20 flex items-center justify-center transition-all duration-300 hover:bg-opacity-30">
                <i class="fas fa-search text-white text-xl"></i>
            </button>
        </div>
    </div>

    <script>
        // DOM Elements
        const micButton = document.getElementById('micButton');
        const cameraButton = document.getElementById('cameraButton');
        const analyzeButton = document.getElementById('analyzeButton');
        const cameraSwitchBtn = document.getElementById('cameraSwitchBtn');
        const cameraView = document.getElementById('cameraView');
        const statusIndicator = document.getElementById('statusIndicator');
        const statusText = document.getElementById('statusText');
        const mainStatusText = document.getElementById('mainStatusText');
        const userSpeechText = document.getElementById('userSpeechText');
        const cameraStatus = document.getElementById('cameraStatus');
        const errorDisplay = document.getElementById('errorDisplay');
        const errorText = document.getElementById('errorText');
        const warningDisplay = document.getElementById('warningDisplay');
        const warningText = document.getElementById('warningText');
        const apiStatus = document.getElementById('apiStatus');
        const conversationHistory = document.getElementById('conversationHistory');
        const conversationList = document.getElementById('conversationList');
        const imagePreviewContainer = document.getElementById('imagePreviewContainer');
        const generatedImage = document.getElementById('generatedImage');
        const downloadImageBtn = document.getElementById('downloadImageBtn');
        const listeningIndicator = document.getElementById('listeningIndicator');
        const ignoringIndicator = document.getElementById('ignoringIndicator');
        
        // App State
        let isListening = false;
        let isIgnoringSystemSounds = false;
        let isCameraActive = false;
        let usingFrontCamera = false;
        let stream = null;
        let isAISpeaking = false;
        let detectionModel = null;
        let conversation = [];
        let currentGeneratedImage = null;
        let silenceTimeout = null;
        let ignoreSystemTimeout = null;
        const SILENCE_TIMEOUT = 60000; // 1 minute
        const SYSTEM_SOUND_IGNORE_TIME = 5000; // 5 seconds
        
        // Google Gemini API Configuration
        const GEMINI_API_KEY = "AIzaSyCAeedrWW42idbmY_6ad7YnWRe9dMgvPDY";
        const GEMINI_MODELS = ["gemini-2.0-flash", "gemini-1.5-flash", "gemini-pro"];
        let currentModelIndex = 0;
        
        // Keywords that trigger camera image capture
        const VISION_KEYWORDS = [
            'see', 'look', 'watch', 'view', 'show', 'display',
            'dekh', 'dikhai', 'dikhta', 'dikhti', 'dikhte',
            'kya hai', 'what is', 'what are', 'what\'s this',
            'hath', 'haath', 'hand', 'hands',
            'age', 'umar', 'guess', 'estimate',
            'describe', 'explain', 'analyze',
            'yeh kya hai', 'yaha kya hai', 'idhar kya hai',
            'abhi', 'currently', 'right now', 'at this moment'
        ];
        
        // Show error message
        function showError(message) {
            errorText.textContent = message;
            errorDisplay.classList.remove('hidden');
            setTimeout(() => {
                errorDisplay.classList.add('hidden');
            }, 5000);
        }

        // Show warning message
        function showWarning(message) {
            warningText.textContent = message;
            warningDisplay.classList.remove('hidden');
            setTimeout(() => {
                warningDisplay.classList.add('hidden');
            }, 3000);
        }

        // Show success message
        function showSuccess(message) {
            apiStatus.innerHTML = `<p class="text-sm">‚úÖ ${message}</p>`;
            setTimeout(() => {
                apiStatus.innerHTML = `<p class="text-sm">üîó Using ${GEMINI_MODELS[currentModelIndex]}</p>`;
            }, 3000);
        }

        // Update main status
        function updateMainStatus(message) {
            mainStatusText.textContent = message;
        }

        // Ignore system sounds for a period
        function ignoreSystemSounds() {
            isIgnoringSystemSounds = true;
            micButton.classList.remove('mic-listening');
            micButton.classList.add('mic-ignoring');
            listeningIndicator.classList.add('hidden');
            ignoringIndicator.classList.remove('hidden');
            
            showWarning("Ignoring system sounds for 5 seconds...");
            
            if (ignoreSystemTimeout) {
                clearTimeout(ignoreSystemTimeout);
            }
            
            ignoreSystemTimeout = setTimeout(() => {
                isIgnoringSystemSounds = false;
                micButton.classList.remove('mic-ignoring');
                if (isListening) {
                    micButton.classList.add('mic-listening');
                    listeningIndicator.classList.remove('hidden');
                }
                ignoringIndicator.classList.add('hidden');
            }, SYSTEM_SOUND_IGNORE_TIME);
        }

        // Reset silence timeout
        function resetSilenceTimeout() {
            if (silenceTimeout) {
                clearTimeout(silenceTimeout);
            }
            silenceTimeout = setTimeout(() => {
                if (isListening && !isAISpeaking) {
                    stopListening();
                    updateMainStatus("Mic stopped due to inactivity");
                    showSuccess("Listening stopped after 1 minute of silence");
                }
            }, SILENCE_TIMEOUT);
        }

        // Add to conversation history with Read More feature
        function addToConversation(userMessage, aiMessage, imageData = null) {
            const conversationId = Date.now();
            const shortMessage = aiMessage.length > 100 ? aiMessage.substring(0, 100) + '...' : aiMessage;
            const hasReadMore = aiMessage.length > 100;
            
            conversation.push({
                id: conversationId,
                user: userMessage,
                ai: aiMessage,
                shortMessage: shortMessage,
                hasReadMore: hasReadMore,
                imageData: imageData,
                timestamp: new Date().toLocaleTimeString()
            });
            
            // Keep only last 5 messages
            if (conversation.length > 5) {
                conversation.shift();
            }
            
            updateConversationUI();
        }

        // Update conversation UI
        function updateConversationUI() {
            conversationList.innerHTML = conversation.map(conv => {
                let content = `
                    <div class="border-b border-white/20 pb-2">
                        <div class="text-cyan-300 text-xs mb-1">
                            <strong>You (${conv.timestamp}):</strong> ${conv.user}
                        </div>
                        <div class="text-green-300 text-xs">
                            <strong>AI:</strong> ${conv.shortMessage}
                            ${conv.hasReadMore ? 
                                `<button onclick="toggleReadMore(${conv.id})" class="read-more-btn">Read More</button>` : 
                                ''
                            }
                        </div>
                        ${conv.imageData ? 
                            `<div class="mt-2">
                                <img src="${conv.imageData}" class="w-20 h-20 rounded object-cover border border-white/20">
                                <button onclick="viewFullImage('${conv.imageData}')" class="read-more-btn mt-1">View Full</button>
                            </div>` : 
                            ''
                        }
                        <div id="fullText-${conv.id}" class="full-conversation hidden">
                            ${conv.ai}
                        </div>
                    </div>
                `;
                return content;
            }).join('');
        }

        // Toggle read more
        function toggleReadMore(convId) {
            const fullTextElement = document.getElementById(`fullText-${convId}`);
            fullTextElement.classList.toggle('hidden');
        }

        // View full image
        function viewFullImage(imageUrl) {
            generatedImage.src = imageUrl;
            imagePreviewContainer.classList.remove('hidden');
            currentGeneratedImage = imageUrl;
        }

        // TensorFlow.js Model Loading
        async function loadDetectionModel() {
            try {
                updateMainStatus("Loading AI vision model...");
                detectionModel = await cocoSsd.load();
                showSuccess("Vision model loaded successfully");
                return true;
            } catch (error) {
                console.error('Error loading detection model:', error);
                showError("Vision model failed to load");
                return false;
            }
        }
        
        // Capture current camera frame and convert to base64
        async function captureCameraFrame() {
            return new Promise((resolve) => {
                const canvas = document.createElement('canvas');
                const ctx = canvas.getContext('2d');
                
                canvas.width = cameraView.videoWidth;
                canvas.height = cameraView.videoHeight;
                
                ctx.drawImage(cameraView, 0, 0, canvas.width, canvas.height);
                
                canvas.toBlob((blob) => {
                    const reader = new FileReader();
                    reader.onload = () => {
                        resolve(reader.result); // base64 data URL
                    };
                    reader.readAsDataURL(blob);
                }, 'image/jpeg', 0.8);
            });
        }
        
        // Check if text contains vision keywords
        function shouldCaptureImage(text) {
            const lowerText = text.toLowerCase();
            return VISION_KEYWORDS.some(keyword => lowerText.includes(keyword.toLowerCase()));
        }
        
        // Speech Recognition with Smart Listening
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        let recognition;
        
        if (SpeechRecognition) {
            recognition = new SpeechRecognition();
            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = 'en-US';
            
            recognition.onstart = () => {
                isListening = true;
                micButton.classList.add('mic-listening');
                listeningIndicator.classList.remove('hidden');
                updateMainStatus("üé§ Listening for your voice...");
                statusText.textContent = "Listening";
                resetSilenceTimeout();
            };
            
            recognition.onresult = (event) => {
                // Skip processing if ignoring system sounds
                if (isIgnoringSystemSounds) {
                    userSpeechText.textContent = "üîá Ignoring system sounds...";
                    return;
                }
                
                // Reset silence timeout on any speech
                resetSilenceTimeout();
                
                let finalTranscript = '';
                let interimTranscript = '';
                
                for (let i = event.resultIndex; i < event.results.length; i++) {
                    const transcript = event.results[i][0].transcript;
                    if (event.results[i].isFinal) {
                        finalTranscript += transcript;
                    } else {
                        interimTranscript += transcript;
                    }
                }
                
                // Show interim results
                if (interimTranscript) {
                    userSpeechText.textContent = `Listening: "${interimTranscript}"`;
                    micButton.classList.add('mic-listening');
                }
                
                // Process final results
                if (finalTranscript) {
                    userSpeechText.textContent = `You said: "${finalTranscript}"`;
                    handleUserSpeech(finalTranscript);
                }
            };
            
            recognition.onerror = (event) => {
                console.error('Speech recognition error:', event.error);
                if (event.error === 'no-speech') {
                    return;
                }
                stopListening();
                showError("Speech recognition error");
                updateMainStatus("Speech recognition error");
            };
            
            recognition.onend = () => {
                // Auto-restart for continuous listening
                if (isListening && !isAISpeaking && !isIgnoringSystemSounds) {
                    setTimeout(() => {
                        if (isListening) {
                            recognition.start();
                        }
                    }, 100);
                }
            };
        }
        
        // Start listening
        function startListening() {
            if (!SpeechRecognition) {
                showError("Speech recognition not supported");
                return;
            }
            
            if (!isListening && !isAISpeaking) {
                try {
                    recognition.start();
                } catch (error) {
                    console.error('Error starting recognition:', error);
                }
            }
        }
        
        // Stop listening
        function stopListening() {
            if (isListening) {
                isListening = false;
                isIgnoringSystemSounds = false;
                recognition.stop();
                micButton.classList.remove('mic-listening', 'mic-active', 'mic-ignoring');
                listeningIndicator.classList.add('hidden');
                ignoringIndicator.classList.add('hidden');
                userSpeechText.textContent = "";
                updateMainStatus("Tap mic to start conversation");
                statusText.textContent = "Ready";
                
                if (silenceTimeout) {
                    clearTimeout(silenceTimeout);
                }
                if (ignoreSystemTimeout) {
                    clearTimeout(ignoreSystemTimeout);
                }
            }
        }
        
        // Camera Management
        async function startCamera(front = false) {
            try {
                if (stream) {
                    stream.getTracks().forEach(track => track.stop());
                }
                
                const constraints = {
                    video: { 
                        facingMode: front ? "user" : "environment",
                        width: { ideal: 1280 },
                        height: { ideal: 720 }
                    },
                    audio: false
                };
                
                stream = await navigator.mediaDevices.getUserMedia(constraints);
                cameraView.srcObject = stream;
                isCameraActive = true;
                
                cameraStatus.textContent = front ? "Front camera active" : "Back camera active";
                showSuccess(`Camera activated (${front ? 'Front' : 'Back'})`);
                
            } catch (error) {
                console.error('Error accessing camera:', error);
                showError("Camera access denied - please allow camera permissions");
                updateMainStatus("Camera access denied");
            }
        }
        
        function switchCamera() {
            usingFrontCamera = !usingFrontCamera;
            startCamera(usingFrontCamera);
        }
        
        // Google Gemini API with Vision
        async function queryGeminiAPI(prompt, includeCameraImage = false) {
            let lastError = null;
            
            for (let i = currentModelIndex; i < GEMINI_MODELS.length; i++) {
                const model = GEMINI_MODELS[i];
                const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${GEMINI_API_KEY}`;
                
                try {
                    let requestBody = {
                        contents: [{
                            parts: []
                        }],
                        generationConfig: {
                            temperature: 0.7,
                            maxOutputTokens: 1000,
                        }
                    };

                    // Add image data if requested
                    if (includeCameraImage && isCameraActive) {
                        const imageData = await captureCameraFrame();
                        requestBody.contents[0].parts.push({
                            inline_data: {
                                mime_type: "image/jpeg",
                                data: imageData.split(',')[1]
                            }
                        });
                    }

                    // Add text prompt
                    requestBody.contents[0].parts.push({
                        text: `You are a helpful AI assistant with vision capabilities. 
                        ${includeCameraImage ? "I'm providing a live camera image. " : ""}
                        Please analyze and respond helpfully to: ${prompt}`
                    });

                    const response = await fetch(apiUrl, {
                        method: "POST",
                        headers: {
                            "Content-Type": "application/json",
                        },
                        body: JSON.stringify(requestBody)
                    });

                    if (!response.ok) {
                        const errorData = await response.json();
                        throw new Error(`Model ${model}: ${errorData.error?.message || 'Unknown error'}`);
                    }

                    const result = await response.json();
                    
                    if (!result.candidates || !result.candidates[0] || !result.candidates[0].content) {
                        throw new Error("Invalid response format from Gemini API");
                    }

                    const generatedText = result.candidates[0].content.parts[0].text;
                    
                    currentModelIndex = i;
                    return generatedText;

                } catch (error) {
                    console.log(`Model ${model} failed:`, error.message);
                    lastError = error;
                    continue;
                }
            }
            
            throw new Error(`All models failed. Last error: ${lastError?.message}`);
        }

        // Generate Image with AI
        async function generateImageWithAI(prompt) {
            try {
                const response = await fetch('https://image.pollinations.ai/prompt/' + encodeURIComponent(prompt), {
                    method: 'GET'
                });

                if (!response.ok) {
                    throw new Error('Image generation failed');
                }

                const blob = await response.blob();
                const imageUrl = URL.createObjectURL(blob);
                return imageUrl;

            } catch (error) {
                console.error('Image generation error:', error);
                return `https://via.placeholder.com/512/667eea/ffffff?text=${encodeURIComponent(prompt)}`;
            }
        }
        
        // Text-to-Speech with System Sound Ignore
        function speakAI(text) {
            return new Promise((resolve) => {
                isAISpeaking = true;
                
                // Start ignoring system sounds before speaking
                ignoreSystemSounds();
                
                // Stop any ongoing speech
                window.speechSynthesis.cancel();
                
                const utterance = new SpeechSynthesisUtterance(text);
                utterance.rate = 0.9;
                utterance.pitch = 1;
                utterance.volume = 1;
                
                utterance.onstart = () => {
                    updateMainStatus("ü§ñ AI is speaking...");
                    statusText.textContent = "AI Speaking";
                };
                
                utterance.onend = () => {
                    isAISpeaking = false;
                    if (isListening) {
                        updateMainStatus("üé§ Listening for your voice...");
                        statusText.textContent = "Listening";
                    } else {
                        updateMainStatus("Tap mic to start conversation");
                        statusText.textContent = "Ready";
                    }
                    resolve();
                };
                
                utterance.onerror = (event) => {
                    console.error('Speech synthesis error:', event);
                    isAISpeaking = false;
                    updateMainStatus("Speech error");
                    statusText.textContent = "Error";
                    resolve();
                };
                
                setTimeout(() => {
                    if (isAISpeaking) {
                        window.speechSynthesis.speak(utterance);
                    }
                }, 100);
            });
        }
        
        // Command Handling with Auto Image Capture
        async function handleUserSpeech(text) {
            try {
                // Stop any ongoing AI speech when user speaks
                if (isAISpeaking) {
                    window.speechSynthesis.cancel();
                    isAISpeaking = false;
                }
                
                updateMainStatus("Processing with Gemini AI...");
                statusText.textContent = "Processing";
                
                let response;
                let generatedImageUrl = null;
                let includeCameraImage = false;
                
                // Auto-detect if camera image should be sent
                if (shouldCaptureImage(text) && isCameraActive) {
                    includeCameraImage = true;
                    cameraStatus.textContent = "üì∏ Capturing image for analysis...";
                }
                
                // Check if we need to generate image
                if (text.toLowerCase().includes('generate') || 
                    text.toLowerCase().includes('create') || 
                    text.toLowerCase().includes('image of') ||
                    text.toLowerCase().includes('draw')) {
                    
                    cameraStatus.textContent = "üé® Generating image...";
                    generatedImageUrl = await generateImageWithAI(text);
                    response = `I've generated an image based on your request: "${text}". You can view and download it below.`;
                    
                } else {
                    // Regular text response with optional camera vision
                    response = await queryGeminiAPI(text, includeCameraImage);
                }
                
                // Add to conversation history
                addToConversation(text, response, generatedImageUrl);
                
                // Show image if generated
                if (generatedImageUrl) {
                    generatedImage.src = generatedImageUrl;
                    imagePreviewContainer.classList.remove('hidden');
                    currentGeneratedImage = generatedImageUrl;
                }
                
                // Speak the response (this will trigger system sound ignore)
                await speakAI(response);
                cameraStatus.textContent = "Camera ready - Show objects and ask questions";
                
            } catch (error) {
                console.error('Error processing speech:', error);
                const fallback = "I'm having trouble processing your request. Please try again.";
                addToConversation(text, fallback);
                await speakAI(fallback);
                cameraStatus.textContent = "Camera ready";
            }
        }

        // Analyze current camera frame
        async function analyzeCamera() {
            if (!isCameraActive) {
                showError("Camera not active");
                return;
            }
            
            try {
                updateMainStatus("Analyzing camera image...");
                const response = await queryGeminiAPI("Describe what you see in this image in detail", true);
                addToConversation("Analyze what you see", response);
                await speakAI(response);
            } catch (error) {
                showError("Analysis failed");
            }
        }
        
        // Event Listeners
        micButton.addEventListener('click', () => {
            if (isListening) {
                stopListening();
            } else {
                startListening();
            }
        });
        
        cameraButton.addEventListener('click', () => {
            if (!isCameraActive) {
                startCamera(false);
            }
        });
        
        cameraSwitchBtn.addEventListener('click', switchCamera);
        
        analyzeButton.addEventListener('click', analyzeCamera);
        
        downloadImageBtn.addEventListener('click', () => {
            if (currentGeneratedImage) {
                const link = document.createElement('a');
                link.href = currentGeneratedImage;
                link.download = 'ai-generated-image.jpg';
                link.click();
            }
        });
        
        // Initialize everything
        window.addEventListener('load', async () => {
            try {
                updateMainStatus("Initializing AI Assistant...");
                
                await loadDetectionModel();
                await startCamera(false);
                
                // Test API connection
                try {
                    await queryGeminiAPI("Hello");
                    showSuccess("Gemini AI connected successfully!");
                    updateMainStatus("Ready! Tap mic to start conversation");
                } catch (apiError) {
                    showError("Gemini API issues - using fallback mode");
                    updateMainStatus("Ready (fallback mode)");
                }
                
            } catch (error) {
                console.error('Initialization error:', error);
                showError("Failed to initialize application");
                updateMainStatus("Initialization failed");
            }
        });

        // Handle page visibility changes
        document.addEventListener('visibilitychange', function() {
            if (document.hidden) {
                window.speechSynthesis.cancel();
                stopListening();
            }
        });

        // Make functions global for HTML onclick
        window.toggleReadMore = toggleReadMore;
        window.viewFullImage = viewFullImage;
    </script>
</body>
</html>
